import os
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage  # Correct import for SystemMessage
from langchain.chains import LLMChain  # Using LLMChain for chaining
from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferWindowMemory
from dotenv import load_dotenv

load_dotenv()


def main():
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """

    groq_api_key = os.environ["API_KEY"]
    model = "llama3-8b-8192"
    # Initialize Groq Langchain chat object and conversation
    groq_chat = ChatGroq(groq_api_key=groq_api_key, model_name=model)

    print("Hello! I'm your friendly Groq chatbot.")

    system_prompt = "You are a friendly conversational chatbot"
    conversational_memory_length = 5  # number of previous messages the chatbot will remember during the conversation

    memory = ConversationBufferWindowMemory(
        k=conversational_memory_length, memory_key="chat_history", return_messages=True
    )

    while True:
        user_question = input("Ask a question: ")

        # If the user has asked a question,
        if user_question:

            # Construct a chat prompt template using various components
            prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessage(content=system_prompt),  # This is the persistent system prompt.
                    MessagesPlaceholder(variable_name="chat_history"),  # Placeholder for chat history.
                    HumanMessagePromptTemplate.from_template("{human_input}"),  # User input template.
                ]
            )

            # Create a conversation chain using LLMChain
            conversation = LLMChain(
                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
                prompt=prompt,  # The constructed prompt template.
                memory=memory,  # The conversational memory object that stores and manages the conversation history.
                verbose=False  # Set to True if you want to see detailed logs for debugging
            )

            # The chatbot's answer is generated by sending the full prompt to the Groq API.
            response = conversation.predict(human_input=user_question)
            print("Chatbot:", response)


if __name__ == "__main__":
    main()
